{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c1d3b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for automation\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "#import for web scrapping\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from os.path import basename\n",
    "import base64\n",
    "\n",
    "#import for image visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "\n",
    "#import for image augmentation\n",
    "import tensorflow.image as tfimg\n",
    "\n",
    "#import for model generation\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "import keras.callbacks\n",
    "import keras.metrics as metrics\n",
    "\n",
    "\n",
    "#extra imports\n",
    "import traceback\n",
    "import time\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9132cf96",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "This section scrapes data from Google Images and stores them in different folders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87aad4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_accumulator:\n",
    "    \n",
    "    def __init__(self, topic, folder):\n",
    "        self.topic = topic\n",
    "        self.folder = folder\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.driver.get('https://www.google.com')\n",
    "\n",
    "    def topic_image(self):\n",
    "        try:\n",
    "            search = self.driver.find_element(By.CLASS_NAME, 'gLFyf')\n",
    "            search.send_keys(self.topic)\n",
    "            search.send_keys(Keys.RETURN)\n",
    "\n",
    "            image_page = self.driver.find_element(By.LINK_TEXT, 'Images')\n",
    "            image_page.click()\n",
    "\n",
    "            try:\n",
    "                if self.driver.find_element(By.CLASS_NAME, \"WhIsp \"):\n",
    "                    a = self.driver.find_element(By.CLASS_NAME, \"WhIsp \")\n",
    "                    a.click()\n",
    "                    b = self.driver.find_elements(By.CLASS_NAME, \"GZcH3e\")\n",
    "                    b[2].click()\n",
    "                    c = self.driver.find_element(By.CLASS_NAME, \"kZgzZe\")\n",
    "                    c.click()\n",
    "            finally:\n",
    "                images = self.driver.find_element(By.CLASS_NAME, 'islrc')\n",
    "                return images\n",
    "\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "    def image_download(self):\n",
    "        current_dir = os.getcwd()\n",
    "        if(not(os.path.exists(os.path.join(os.getcwd(), self.folder)))):\n",
    "            os.mkdir(os.path.join(os.getcwd(), self.folder))\n",
    "            os.chdir(os.path.join(os.getcwd(), self.folder))\n",
    "        else:\n",
    "            os.chdir(os.path.join(os.getcwd(), self.folder))\n",
    "        \n",
    "        page = self.topic_image()\n",
    "        i = 0\n",
    "        elements = page.get_attribute('outerHTML') #gives exact HTML content of the element\n",
    "        soup = bs(elements,'html.parser')\n",
    "        img = soup.findAll('img',{\"src\":True, \"height\":True, \"width\":True})\n",
    "        print(f\"Total Images found: {len(img)}\")\n",
    "        \n",
    "        for link in img:\n",
    "            png = link[\"src\"]\n",
    "            if r\"data:image\" in png:\n",
    "                data = png.split(',')[1]\n",
    "                i = i+1\n",
    "                with open(f\"data{i}.png\", \"wb\") as f:\n",
    "                    f.write(base64.b64decode(data))\n",
    "            else:\n",
    "                i = i+1\n",
    "                with open(f\"file{i}.png\", \"wb\") as f:\n",
    "                    f.write(requests.get(png).content)\n",
    "        os.chdir(current_dir)\n",
    "#         time.sleep(500)\n",
    "        self.driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd069087",
   "metadata": {},
   "source": [
    "# Dataset Noise Addition\n",
    "Adding noisy images to the dataset to get more robust model and also increase the number of dataset values to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34ab96ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class image_augmentation:\n",
    "    \n",
    "    def flip(self, picture):\n",
    "        x = tf.image.random_flip_left_right(picture)\n",
    "        x = tf.image.random_flip_up_down(picture)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def color(self, picture):\n",
    "        x = tf.image.random_hue(picture, 0.5)\n",
    "        x = tf.image.random_saturation(picture, 0.3, 1.6)\n",
    "        x = tf.image.random_brightness(picture, 0.8)\n",
    "        x = tf.image.random_contrast(picture, 0.7, 1.3)\n",
    "        return x\n",
    "\n",
    "    def rotate(self, picture):\n",
    "        \n",
    "        return tf.image.rot90(picture, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
    "\n",
    "    \n",
    "    def zoom(self, picture):\n",
    "\n",
    "        # Generate 20 crop settings, ranging from a 1% to 20% crop.\n",
    "        scales = list(np.arange(0.8, 1.0, 0.01))\n",
    "        boxes = np.zeros((len(scales), 4))\n",
    "\n",
    "        for i, scale in enumerate(scales):\n",
    "            x1 = y1 = 0.5 - (0.5 * scale)\n",
    "            x2 = y2 = 0.5 + (0.5 * scale)\n",
    "            boxes[i] = [x1, y1, x2, y2]\n",
    "\n",
    "        def random_crop(img):\n",
    "            # Create different crops for an image\n",
    "            crops = tf.image.crop_and_resize([img], boxes=boxes, box_indices=np.zeros(len(scales)), crop_size=(32, 32))\n",
    "            # Return a random crop\n",
    "            \n",
    "            return crops[tf.random.uniform(shape=[], minval=0, maxval=len(scales), dtype=tf.int32)]\n",
    "\n",
    "        choice = tf.random.uniform(shape=[], minval=0., maxval=1., dtype=tf.float32)\n",
    "\n",
    "        # Only apply cropping 50% of the time\n",
    "        return tf.cond(choice < 0.5, lambda: picture, lambda: random_crop(picture))\n",
    "\n",
    "    \n",
    "    # Add augmentations\n",
    "    def image_augmentor(self, picture):\n",
    "        \n",
    "        functions = [self.flip, self.color, self.rotate]\n",
    "        photo = picture\n",
    "        for func in functions:\n",
    "            photo = func(photo)\n",
    "        \n",
    "        return photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e471388b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'with' statement on line 24 (4120643002.py, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[17], line 25\u001b[1;36m\u001b[0m\n\u001b[1;33m    img = augment.image_augmentor(plt.imread(i, format=\"PNG\"))\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'with' statement on line 24\n"
     ]
    }
   ],
   "source": [
    "class data_generator:\n",
    "    \n",
    "    def __init__(self, item, loc):\n",
    "        self.topic = item\n",
    "        self.folder = loc\n",
    "        self.current_dir = os.getcwd()\n",
    "    \n",
    "    \n",
    "    def accumulate(self):\n",
    "        acc = data_accumulator(topic=self.topic, folder=self.folder)\n",
    "        acc.image_download()\n",
    "    \n",
    "    def set_path(self):\n",
    "        current_dir = os.getcwd()\n",
    "        return os.chdir(os.path.join(current_dir, self.folder))\n",
    "    \n",
    "    def augment(self):\n",
    "        augment = image_augmentation()\n",
    "        files = os.listdir(self.set_path())\n",
    "        j = 0\n",
    "    \n",
    "        for i in files:\n",
    "            if \"data\" in i:\n",
    "                with open(i, \"rb\") as f:\n",
    "                img = augment.image_augmentor(plt.imread(i, format=\"PNG\"))\n",
    "            \n",
    "            with open(f\"aug{j}.png\", \"wb\") as f:\n",
    "                j = j+1\n",
    "                f.write(tf.image.encode_png(img).numpy())    \n",
    "            \n",
    "        os.chdir(self.current_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8134896",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"E:\\Github\\Deep-Learning\\ImageClassifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9961b978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Github\\\\Deep-Learning\\\\ImageClassifier'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e950727",
   "metadata": {},
   "source": [
    "# Image Classifier\n",
    "This section would consist of the Image Classifier model set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db071f5d",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "Creating a pipeline to fetch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9199e92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f928de63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2df4e29c",
   "metadata": {},
   "source": [
    "# Driver Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f62cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    a = data_generator(\"Mikel Arteta\", \"brev\")\n",
    "    a.accumulate()\n",
    "    a.augment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14934d44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
